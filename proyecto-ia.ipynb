{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5175158,"sourceType":"datasetVersion","datasetId":3008205}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## *Install Libraries*","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow==2.15.0\n!pip install mne==1.6.0\n!pip install braindecode===0.7\n!pip install -q -U keras-tuner\n!pip install git+https://github.com/Anlopezbo/EEGVAENL.git\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:35:23.612599Z","iopub.execute_input":"2024-10-07T19:35:23.613297Z","iopub.status.idle":"2024-10-07T19:37:36.610735Z","shell.execute_reply.started":"2024-10-07T19:35:23.613250Z","shell.execute_reply":"2024-10-07T19:37:36.609710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Import Libraries*","metadata":{}},{"cell_type":"code","source":"from scipy.signal import resample\nfrom scipy.signal import freqz, filtfilt, resample\nfrom scipy.signal import butter as bw\nimport pandas as pd\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom braindecode.datasets.moabb import MOABBDataset\nfrom gcpds.databases import GIGA_MI_ME\nfrom braindecode.preprocessing.preprocess import (exponential_moving_standardize, preprocess, Preprocessor, scale)\nfrom braindecode.preprocessing.windowers import create_windows_from_events\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,Callback, CSVLogger, ReduceLROnPlateau\nimport time\nimport csv\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,KFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import Conv2D, AveragePooling2D,Conv2DTranspose\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Input, Flatten, Lambda\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers.experimental.preprocessing import Resizing\nfrom tensorflow.keras.losses import CategoricalCrossentropy, binary_crossentropy, Loss\nfrom tensorflow.keras.regularizers import l1_l2\nfrom tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\nfrom tensorflow.keras.layers import SpatialDropout2D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import Dropout, Add, Lambda, Permute\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import BatchNormalization, Reshape\nfrom tensorflow.keras.layers import SpatialDropout2D, UpSampling2D\nfrom keras_tuner import HyperParameters, GridSearch\nfrom keras_tuner.tuners import RandomSearch, BayesianOptimization\nfrom keras_tuner import RandomSearch\nfrom keras_tuner import Objective\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.python.keras.utils import losses_utils\nfrom keras_tuner import BayesianOptimization as OriginalBayesianOptimization\nfrom keras_tuner.oracles import BayesianOptimizationOracle as OriginalBayesianOptimizationOracle\nimport sklearn\nimport sklearn.exceptions\nimport sklearn.gaussian_process\nfrom keras_tuner.src.api_export import keras_tuner_export\nfrom keras_tuner.src.engine import hyperparameters as hp_module\nfrom keras_tuner.src.engine import oracle as oracle_module\nfrom keras_tuner.src.engine import trial as trial_module\nfrom keras_tuner.src.engine import tuner as tuner_module","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:36.612919Z","iopub.execute_input":"2024-10-07T19:37:36.613258Z","iopub.status.idle":"2024-10-07T19:37:48.580636Z","shell.execute_reply.started":"2024-10-07T19:37:36.613220Z","shell.execute_reply":"2024-10-07T19:37:48.579713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Load Data*","metadata":{}},{"cell_type":"code","source":"channels = ['Fp1','Fpz','Fp2',\n            'AF7','AF3','AFz','AF4','AF8',\n            'F7','F5','F3','F1','Fz','F2','F4','F6','F8',\n            'FT7','FC5','FC3','FC1','FCz','FC2','FC4','FC6','FT8',\n            'T7','C5','C3','C1','Cz','C2','C4','C6','T8',\n            'TP7','CP5','CP3','CP1','CPz','CP2','CP4','CP6','TP8',\n            'P9','P7','P5','P3','P1','Pz','P2','P4','P6','P8','P10',\n            'PO7','PO3','POz','PO4','PO8',\n            'O1','Oz','O2',\n            'Iz']\n\nareas = {\n    'Frontal': ['Fpz', 'AFz', 'Fz', 'FCz'],\n    'Frontal Right': ['Fp2','AF4','AF8','F2','F4','F6','F8',],\n    'Central Right': ['FC2','FC4','FC6','FT8','C2','C4','C6','T8','CP2','CP4','CP6','TP8',],\n    'Posterior Right': ['P2','P4','P6','P8','P10','PO4','PO8','O2',],\n    #'Central': ['Cz'],\n    'Posterior': ['CPz','Pz', 'Cz','POz','Oz','Iz',],\n    'Posterior Left': ['P1','P3','P5','P7','P9','PO3','PO7','O1',],\n    'Central Left': ['FC1','FC3','FC5','FT7','C1','C3','C5','T7','CP1','CP3','CP5','TP7',],\n    'Frontal Left': ['Fp1','AF3','AF7','F1','F3','F5','F7',],\n}\n\narcs = [\n    #'hemispheres',\n    'areas',\n    'channels',\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.582018Z","iopub.execute_input":"2024-10-07T19:37:48.582775Z","iopub.status.idle":"2024-10-07T19:37:48.594222Z","shell.execute_reply.started":"2024-10-07T19:37:48.582737Z","shell.execute_reply":"2024-10-07T19:37:48.593247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = GIGA_MI_ME('/kaggle/input/giga-science-gcpds/GIGA_MI_ME')\nload_args = dict(db = db,\n                 eeg_ch_names = channels,\n                 fs = db.metadata['sampling_rate'],\n                 f_bank = np.asarray([[4., 40.]]),\n                 vwt = np.asarray([[2.5, 5]]),\n                 new_fs = 128.)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.596386Z","iopub.execute_input":"2024-10-07T19:37:48.596771Z","iopub.status.idle":"2024-10-07T19:37:48.615828Z","shell.execute_reply.started":"2024-10-07T19:37:48.596727Z","shell.execute_reply":"2024-10-07T19:37:48.614875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def butterworth_digital_filter(X, N, Wn, btype, fs, axis=-1, padtype=None, padlen=0, method='pad', irlen=None):\n  \"\"\"\n  Apply digital butterworth filter\n  INPUT\n  ------\n  1. X: (D array)\n    array with signals.\n  2. N: (int+)\n    The order of the filter.\n  3. Wn: (float+ or 1D array)\n    The critical frequency or frequencies. For lowpass and highpass filters, Wn is a scalar; for bandpass and bandstop filters, Wn is a length-2 vector.\n    For a Butterworth filter, this is the point at which the gain drops to 1/sqrt(2) that of the passband (the “-3 dB point”).\n    If fs is not specified, Wn units are normalized from 0 to 1, where 1 is the Nyquist frequency (Wn is thus in half cycles / sample and defined as 2*critical frequencies / fs). If fs is specified, Wn is in the same units as fs.\n  4. btype: (str) {‘lowpass’, ‘highpass’, ‘bandpass’, ‘bandstop’}\n    The type of filter\n  5. fs: (float+)\n    The sampling frequency of the digital system.\n  6. axis: (int), Default=1.\n    The axis of x to which the filter is applied.\n  7. padtype: (str) or None, {'odd', 'even', 'constant'}\n    This determines the type of extension to use for the padded signal to which the filter is applied. If padtype is None, no padding is used. The default is ‘odd’.\n  8. padlen: (int+) or None, Default=0\n    The number of elements by which to extend x at both ends of axis before applying the filter. This value must be less than x.shape[axis] - 1. padlen=0 implies no padding.\n  9. method: (str), {'pad', 'gust'}\n    Determines the method for handling the edges of the signal, either “pad” or “gust”. When method is “pad”, the signal is padded; the type of padding is determined by padtype\n    and padlen, and irlen is ignored. When method is “gust”, Gustafsson’s method is used, and padtype and padlen are ignored.\n  10. irlen: (int) or None, Default=nONE\n    When method is “gust”, irlen specifies the length of the impulse response of the filter. If irlen is None, no part of the impulse response is ignored.\n    For a long signal, specifying irlen can significantly improve the performance of the filter.\n  OUTPUT\n  ------\n  X_fil: (D array)\n    array with filtered signals.\n  \"\"\"\n  b, a = bw(N, Wn, btype, analog=False, output='ba', fs=fs)\n  return filtfilt(b, a, X, axis=axis, padtype=padtype, padlen=padlen, method=method, irlen=irlen)\n\nclass TimeFrequencyRpr(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Time frequency representation of EEG signals.\n\n  Parameters\n  ----------\n    1. sfreq:  (float) Sampling frequency in Hz.\n    2. f_bank: (2D array) Filter banks Frequencies. Default=None\n    3. vwt:    (2D array) Interest time windows. Default=None\n  Methods\n  -------\n    1. fit(X, y=None)\n    2. transform(X, y=None)\n  \"\"\"\n  def __init__(self, sfreq, f_bank=None, vwt=None):\n    self.sfreq = sfreq\n    self.f_bank = f_bank\n    self.vwt = vwt\n# ------------------------------------------------------------------------------\n\n  def _validation_param(self):\n    \"\"\"\n    Validate Time-Frequency characterization parameters.\n    INPUT\n    -----\n      1. self\n    ------\n      2. None\n    \"\"\"\n    if self.sfreq <= 0:\n      raise ValueError('Non negative sampling frequency is accepted')\n\n\n    if self.f_bank is None:\n      self.flag_f_bank = False\n    elif self.f_bank.ndim != 2:\n      raise ValueError('Band frequencies have to be a 2D array')\n    else:\n      self.flag_f_bank = True\n\n    if self.vwt is None:\n      self.flag_vwt = False\n    elif self.vwt.ndim != 2:\n      raise ValueError('Time windows have to be a 2D array')\n    else:\n      self.flag_vwt = True\n\n# ------------------------------------------------------------------------------\n  def _filter_bank(self, X):\n    \"\"\"\n    Filter bank Characterization.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n    OUTPUT\n    ------\n      1. X_f: (4D array) set of filtered EEG signals, shape (trials, channels, time_samples, frequency_bands)\n    \"\"\"\n    X_f = np.zeros((X.shape[0], X.shape[1], X.shape[2], self.f_bank.shape[0])) #epochs, Ch, Time, bands\n    for f in np.arange(self.f_bank.shape[0]):\n      X_f[:,:,:,f] = butterworth_digital_filter(X, N=5, Wn=self.f_bank[f], btype='bandpass', fs=self.sfreq)\n    return X_f\n\n# ------------------------------------------------------------------------------\n  def _sliding_windows(self, X):\n    \"\"\"\n    Sliding Windows Characterization.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n    OUTPUT\n    ------\n      1. X_w: (4D array) shape (trials, channels, window_time_samples, number_of_windows)\n    \"\"\"\n    window_lenght = int(self.sfreq*self.vwt[0,1] - self.sfreq*self.vwt[0,0])\n    X_w = np.zeros((X.shape[0], X.shape[1], window_lenght, self.vwt.shape[0]))\n    for w in np.arange(self.vwt.shape[0]):\n        X_w[:,:,:,w] = X[:,:,int(self.sfreq*self.vwt[w,0]):int(self.sfreq*self.vwt[w,1])]\n    return X_w\n\n# ------------------------------------------------------------------------------\n  def fit(self, X, y=None):\n    \"\"\"\n    fit.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, time_samples)\n      2. y: (1D array) target labels. Default=None\n    OUTPUT\n    ------\n      1. None\n    \"\"\"\n    pass\n\n# ------------------------------------------------------------------------------\n  def transform(self, X, y=None):\n    \"\"\"\n    Time frequency representation of EEG signals.\n    INPUT\n    -----\n      1. X: (3D array) set of EEG signals, shape (trials, channels, times)\n    OUTPUT\n    ------\n      1. X_wf: (5D array) Time-frequency representation of EEG signals, shape (trials, channels, window_time_samples, number_of_windows, frequency_bands)\n    \"\"\"\n    self._validation_param()     #Validate sfreq, f_freq, vwt\n\n    #Avoid edge effects of digital filter, 1st:fbk, 2th:vwt\n    if self.flag_f_bank:\n        X_f = self._filter_bank(X)\n    else:\n        X_f = X[:,:,:,np.newaxis]\n\n    if self.flag_vwt:\n      X_wf = []\n      for f in range(X_f.shape[3]):\n        X_wf.append(self._sliding_windows(X_f[:,:,:,f]))\n      X_wf = np.stack(X_wf, axis=-1)\n    else:\n      X_wf = X_f[:,:,:,np.newaxis,:]\n\n    return X_wf","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.616903Z","iopub.execute_input":"2024-10-07T19:37:48.617207Z","iopub.status.idle":"2024-10-07T19:37:48.640697Z","shell.execute_reply.started":"2024-10-07T19:37:48.617174Z","shell.execute_reply":"2024-10-07T19:37:48.639848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_GIGA(db,\n              sbj,\n              eeg_ch_names,\n              fs,\n              f_bank,\n              vwt,\n              new_fs,\n              run=None):\n\n    index_eeg_chs = db.format_channels_selectors(channels = eeg_ch_names) - 1\n\n    tf_repr = TimeFrequencyRpr(sfreq = fs, f_bank = f_bank, vwt = vwt)\n\n    db.load_subject(sbj)\n    if run == None:\n        X, y = db.get_data(classes = ['left hand mi', 'right hand mi']) #Load MI classes, all channels {EEG}, reject bad trials, uV\n    else:\n        X, y = db.get_run(run, classes = ['left hand mi', 'right hand mi']) #Load MI classes, all channels {EEG}, reject bad trials, uV\n    X = X[:, index_eeg_chs, :] #spatial rearrangement\n    X = np.squeeze(tf_repr.transform(X))\n    #Resampling\n    if new_fs == fs:\n        pass#print('No resampling, since new sampling rate same.')\n    else:\n        print(\"Resampling from {:f} to {:f} Hz.\".format(fs, new_fs))\n        X = resample(X, int((X.shape[-1]/fs)*new_fs), axis = -1)\n\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.641627Z","iopub.execute_input":"2024-10-07T19:37:48.642001Z","iopub.status.idle":"2024-10-07T19:37:48.653994Z","shell.execute_reply.started":"2024-10-07T19:37:48.641949Z","shell.execute_reply":"2024-10-07T19:37:48.653069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *NORMALIZED LOSSES FUNCTIONS FOR DEEP LEARNING:*","metadata":{}},{"cell_type":"markdown","source":" Dado un conjunto de datos de $ K-clases $ con etiquetas ruidosas, denotado como $ D = \\{(x, y^{(i)})\\}_{i=1}^n $, donde $ x \\in X \\subset \\mathbb{R}^d $ representa una muestra, $ y \\in Y = \\{1, \\ldots, K\\}$ es su etiqueta anotada (que puede ser incorrecta), consideramos la distribución sobre diferentes etiquetas para cada muestra $ x{\\text{}}$, denotada por $ q(k|x) $. Se cumple que $ \\sum_{k=1}^K q(k|x) = 1 $.\n\n En este documento, nos enfocamos en el caso común donde hay una única etiqueta $ y{\\text{}} $ para $ x{\\text{}} $: es decir, $ q(y|x) = 1 $ y $ q(k \\neq y|x) = 0 $. En este caso, $ q $ es simplemente la codificación one-hot de la etiqueta.\n","metadata":{}},{"cell_type":"markdown","source":"#### *NORMALIZED LOSS*\n$$\nL_{\\text{norm}} = \\frac{L(f(x), y)}{\\sum_{j=1}^K L(f(x), j)}\n$$\n\n#### *CROSS ENTROPY:*\n\n$$\nCE = {-\\sum_{k=1}^{K}\\lg{q(k\\mid X)}*\\lg{P(k\\mid X)}}\n$$\n\n#### *NORMALIZED CROSS ENTROPY:*\n\n$$\nNCE = \\frac{-\\sum_{k=1}^{K}\\lg{q(k\\mid X)}*\\lg{P(k\\mid X)}}{-\\sum_{j=1}^{K}\\sum_{k=0}^{K}\\lg{q(y=j\\mid X)}*\\lg{P(k\\mid X)}}\n$$","metadata":{}},{"cell_type":"markdown","source":"Denotamos la etiqueta verdadera de $x$ como $y^*$. Aunque las etiquetas ruidosas pueden surgir de diferentes maneras, una suposición común es que, dado las etiquetas verdaderas, el ruido es condicionalmente independiente de las entradas, es decir, $q(y = k \\mid y^* = j, x) = q(y = k \\mid y^* = j)$.\n\nBajo esta suposición, el ruido en las etiquetas puede ser simétrico (o uniforme) o asimétrico (o condicional a la clase). Denotamos la tasa general de ruido por $\\eta \\in [0, 1]$ y la tasa de ruido específica de clase de $j$ a $k$ por $\\eta_{jk}$. Entonces, para ruido simétrico, $\\eta_{jk} = \\frac{\\eta}{K-1}$ para $j \\neq k$ y $\\eta_{jk} = 1 - \\eta$ para $j = k$.\n\nPara ruido asimétrico, $\\eta_{jk}$ está condicionado tanto a la clase verdadera $j$ como a la clase mal etiquetada $k$.","metadata":{}},{"cell_type":"markdown","source":"La clasificación consiste en aprender una función $f : X \\to Y$ que mapea el espacio de entrada al espacio de etiquetas.\n\n**Lema 1.** En un problema de clasificación multiclase, cualquier función de pérdida normalizada $L_{\\text{norm}}$ es tolerante al ruido bajo ruido de etiqueta simétrico (o uniforme), si la tasa de ruido $\\eta < \\frac{K-1}{K}$.\n\n**Lema 2.** En un problema de clasificación multiclase, dado que $R(f^*) = 0$ y $0 \\leq L_{\\text{norm}}(f(x), k) \\leq \\frac{1}{K-1}$, $\\forall k$, cualquier función de pérdida normalizada $L_{\\text{norm}}$ es tolerante al ruido bajo ruido de etiqueta asimétrico (o condicional a la clase), si la tasa de ruido $\\eta_{jk} < 1 - \\eta_y$.\n\nDenotamos el riesgo del clasificador $f$ bajo etiquetas limpias como $R(f) = \\mathbb{E}_{x,y^*} [L_{\\text{norm}}]$, y el riesgo bajo la tasa de ruido de etiqueta $\\eta$ como $R_{\\eta}(f) = \\mathbb{E}_{x,y} [L_{\\text{norm}}]$. Sean $f^*$ y $f^*_{\\eta}$ los minimizadores globales de $R(f)$ y $R_{\\eta}(f)$, respectivamente. Necesitamos probar que $f^*$ también es un minimizador global del riesgo ruidoso $R_{\\eta}(f)$ para que $L$ sea robusta. Las condiciones de la tasa de ruido en el Lema 1 ($\\eta < \\frac{K-1}{K}$) y en el Lema 2 ($\\eta_{jk} < 1 - \\eta_y$) generalmente requieren que las etiquetas correctas sigan siendo la mayoría de la clase. En el Lema 2, la condición restrictiva $R(f^*) = 0$ puede no ser satisfecha en la práctica (por ejemplo, las clases pueden no ser completamente separables); sin embargo, aún se puede lograr una buena robustez empírica. Mientras que la condición $0 \\leq L_{\\text{norm}}(f(x), k) \\leq \\frac{1}{K-1}$, $\\forall k$ puede ser fácilmente satisfecha por una función de pérdida típica.\n","metadata":{}},{"cell_type":"markdown","source":"## *Custom Loss (NCE)*","metadata":{}},{"cell_type":"code","source":"class NormalizedCrossEntropy(tf.keras.losses.Loss):\n    def __init__(self, num_classes, scale=1.0, name=\"normalized_cross_entropy\"):\n        super(NormalizedCrossEntropy, self).__init__(name=name)\n        self.num_classes = num_classes\n        self.scale = scale\n\n    def call(self, y_true, y_pred):\n        # Aplicar log_softmax a las predicciones\n        # y_pred = tf.nn.log_softmax(y_pred, axis=1)\n        y_true = tf.cast(y_true, tf.float32)\n\n        # Convertir etiquetas verdaderas a one-hot\n        # y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), self.num_classes)\n        #y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n\n        # Calcular la entropía cruzada normalizada\n        nce = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1) / (-tf.reduce_sum(tf.math.log(y_pred), axis=1))\n\n        # Escalar y devolver la pérdida media\n        # return self.scale * tf.reduce_mean(nce)\n        return tf.reduce_mean(nce)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.655213Z","iopub.execute_input":"2024-10-07T19:37:48.655528Z","iopub.status.idle":"2024-10-07T19:37:48.666598Z","shell.execute_reply.started":"2024-10-07T19:37:48.655496Z","shell.execute_reply":"2024-10-07T19:37:48.665724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Models*","metadata":{}},{"cell_type":"code","source":"def EEGNet(nb_classes, Chans = 64, Samples = 320,\n             dropoutRate = 0.5, kernLength = 64, F1 = 8,\n             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n    \"\"\" Keras Implementation of EEGNet\n    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n    Note that this implements the newest version of EEGNet and NOT the earlier\n    version (version v1 and v2 on arxiv). We strongly recommend using this\n    architecture as it performs much better and has nicer properties than\n    our earlier version. For example:\n\n        1. Depthwise Convolutions to learn spatial filters within a\n        temporal convolution. The use of the depth_multiplier option maps\n        exactly to the number of spatial filters learned within a temporal\n        filter. This matches the setup of algorithms like FBCSP which learn\n        spatial filters within each filter in a filter-bank. This also limits\n        the number of free parameters to fit when compared to a fully-connected\n        convolution.\n\n        2. Separable Convolutions to learn how to optimally combine spatial\n        filters across temporal bands. Separable Convolutions are Depthwise\n        Convolutions followed by (1x1) Pointwise Convolutions.\n\n\n    While the original paper used Dropout, we found that SpatialDropout2D\n    sometimes produced slightly better results for classification of ERP\n    signals. However, SpatialDropout2D significantly reduced performance\n    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n    the default Dropout in most cases.\n\n    Assumes the input signal is sampled at 128Hz. If you want to use this model\n    for any other sampling rate you will need to modify the lengths of temporal\n    kernels and average pooling size in blocks 1 and 2 as needed (double the\n    kernel lengths for double the sampling rate, etc). Note that we haven't\n    tested the model performance with this rule so this may not work well.\n\n    The model with default parameters gives the EEGNet-8,2 model as discussed\n    in the paper. This model should do pretty well in general, although it is\n\tadvised to do some model searching to get optimal performance on your\n\tparticular dataset.\n    We set F2 = F1 * D (number of input filters = number of output filters) for\n    the SeparableConv2D layer. We haven't extensively tested other values of this\n    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n    overcomplete). We believe the main parameters to focus on are F1 and D.\n    Inputs:\n\n      nb_classes      : int, number of classes to classify\n      Chans, Samples  : number of channels and time points in the EEG data\n      dropoutRate     : dropout fraction\n      kernLength      : length of temporal convolution in first layer. We found\n                        that setting this to be half the sampling rate worked\n                        well in practice. For the SMR dataset in particular\n                        since the data was high-passed at 4Hz we used a kernel\n                        length of 32.\n      F1, F2          : number of temporal filters (F1) and number of pointwise\n                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D.\n      D               : number of spatial filters to learn within each temporal\n                        convolution. Default: D = 2\n      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n    \"\"\"\n\n    if dropoutType == 'SpatialDropout2D':\n        dropoutType = SpatialDropout2D\n    elif dropoutType == 'Dropout':\n        dropoutType = Dropout\n    else:\n        raise ValueError('dropoutType must be one of SpatialDropout2D '\n                         'or Dropout, passed as a string.')\n\n    input1   = Input(shape = (Chans, Samples, 1))\n\n    ##################################################################\n    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n                                   name='Conv2D_1',\n                                   input_shape = (Chans, Samples, 1),\n                                   use_bias = False)(input1)\n    block1       = BatchNormalization()(block1)\n    block1       = DepthwiseConv2D((Chans, 1), use_bias = False,\n                                   name='Depth_wise_Conv2D_1',\n                                   depth_multiplier = D,\n                                   depthwise_constraint = max_norm(1.))(block1)\n    block1       = BatchNormalization()(block1)\n    block1       = Activation('elu')(block1)\n    block1       = AveragePooling2D((1, 4))(block1)\n    block1       = dropoutType(dropoutRate)(block1)\n\n    block2       = SeparableConv2D(F2, (1, 16),\n                                   name='Separable_Conv2D_1',\n                                   use_bias = False, padding = 'same')(block1)\n    block2       = BatchNormalization()(block2)\n    block2       = Activation('elu')(block2)\n    block2       = AveragePooling2D((1, 8))(block2)\n    block2       = dropoutType(dropoutRate)(block2)\n\n    flatten      = Flatten(name = 'flatten')(block2)\n\n    dense        = Dense(nb_classes, name = 'output',\n                         kernel_constraint = max_norm(norm_rate))(flatten)\n    softmax      = Activation('softmax', name = 'out_activation')(dense)\n\n    return Model(inputs=input1, outputs=softmax)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.667945Z","iopub.execute_input":"2024-10-07T19:37:48.668423Z","iopub.status.idle":"2024-10-07T19:37:48.683551Z","shell.execute_reply.started":"2024-10-07T19:37:48.668391Z","shell.execute_reply":"2024-10-07T19:37:48.682738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sampling(Layer):\n    def call(self, inputs):\n        mean, log_var = inputs\n        batch = tf.shape(mean)[0]\n        dim = tf.shape(mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return mean + tf.exp(0.5 * log_var) * epsilon\n\ndef EEGNetVAE(nb_classes, Chans = 64, Samples = 320,\n             dropoutRate = 0.5, kernLength = 128, F1 = 8,\n             D = 2, F2 = 16, norm_rate = 0.25, codings_size = 10, dropoutType = 'Dropout', om = 0.5):\n\n\n\n    \"\"\" Keras Implementation of EEGNet\n    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n    Note that this implements the newest version of EEGNet and NOT the earlier\n    version (version v1 and v2 on arxiv). We strongly recommend using this\n    architecture as it performs much better and has nicer properties than\n    our earlier version. For example:\n\n        1. Depthwise Convolutions to learn spatial filters within a\n        temporal convolution. The use of the depth_multiplier option maps\n        exactly to the number of spatial filters learned within a temporal\n        filter. This matches the setup of algorithms like FBCSP which learn\n        spatial filters within each filter in a filter-bank. This also limits\n        the number of free parameters to fit when compared to a fully-connected\n        convolution.\n\n        2. Separable Convolutions to learn how to optimally combine spatial\n        filters across temporal bands. Separable Convolutions are Depthwise\n        Convolutions followed by (1x1) Pointwise Convolutions.\n\n\n    While the original paper used Dropout, we found that SpatialDropout2D\n    sometimes produced slightly better results for classification of ERP\n    signals. However, SpatialDropout2D significantly reduced performance\n    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n    the default Dropout in most cases.\n\n    Assumes the input signal is sampled at 128Hz. If you want to use this model\n    for any other sampling rate you will need to modify the lengths of temporal\n    kernels and average pooling size in blocks 1 and 2 as needed (double the\n    kernel lengths for double the sampling rate, etc). Note that we haven't\n    tested the model performance with this rule so this may not work well.\n\n    The model with default parameters gives the EEGNet-8,2 model as discussed\n    in the paper. This model should do pretty well in general, although it is\n\tadvised to do some model searching to get optimal performance on your\n\tparticular dataset.\n    We set F2 = F1 * D (number of input filters = number of output filters) for\n    the SeparableConv2D layer. We haven't extensively tested other values of this\n    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n    overcomplete). We believe the main parameters to focus on are F1 and D.\n    Inputs:\n\n      nb_classes      : int, number of classes to classify\n      Chans, Samples  : number of channels and time points in the EEG data\n      dropoutRate     : dropout fraction\n      kernLength      : length of temporal convolution in first layer. We found\n                        that setting this to be half the sampling rate worked\n                        well in practice. For the SMR dataset in particular\n                        since the data was high-passed at 4Hz we used a kernel\n                        length of 32.\n      F1, F2          : number of temporal filters (F1) and number of pointwise\n                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D.\n      D               : number of spatial filters to learn within each temporal\n                        convolution. Default: D = 2\n      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n    \"\"\"\n\n    if dropoutType == 'SpatialDropout2D':\n        dropoutType = SpatialDropout2D\n    elif dropoutType == 'Dropout':\n        dropoutType = Dropout\n    else:\n        raise ValueError('dropoutType must be one of SpatialDropout2D '\n                         'or Dropout, passed as a string.')\n\n    input1   = Input(shape = (Chans, Samples, 1))\n\n    ##################################################################\n    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n                                   name='Conv2D_1',\n                                   input_shape = (Chans, Samples, 1),\n                                   use_bias = False)(input1)\n\n    block1       = BatchNormalization()(block1)\n    block1       = Conv2D(F1*D,(Chans, 1), use_bias = False, name = 'Depth_wise_remplace')(block1)\n    #block1       = DepthwiseConv2D((Chans, 1), use_bias = False,\n                                   #name='Depth_wise_Conv2D_1',\n                                   #depth_multiplier = D,\n                                   #depthwise_constraint = max_norm(1.))(block1)\n    block1       = BatchNormalization()(block1)\n    block1       = Activation('elu')(block1)\n    block1       = AveragePooling2D((1, 4))(block1)\n    block1       = dropoutType(dropoutRate)(block1)\n\n    #block2       = SeparableConv2D(F2, (1, 16),\n                                   #name='Separable_Conv2D_1',\n                                   #use_bias = False, padding = 'same')(block1)\n    block2       = Conv2D(F1*D, (1,32), padding = 'same', name = 'Separable_remplace_1', use_bias = False)(block1)\n    block2       = Conv2D(F2, (1,1), name = 'Separable_remplace_2', use_bias = False)(block2)\n    block2       = BatchNormalization()(block2)\n    block2       = Activation('elu')(block2)\n    block2       = AveragePooling2D((1, 8))(block2)\n    block2       = dropoutType(dropoutRate)(block2)\n    flatten_1      = Flatten(name = 'flatten_1')(block2)\n    \n    codings_mean = keras.layers.Dense(codings_size, name='codings_mean_1')(flatten_1)\n    codings_log_var = keras.layers.Dense(codings_size, name='codings_log_var_1')(flatten_1)\n    codings = Sampling(name='sampling')([codings_mean, codings_log_var])\n\n\n    # SeparableDecoder\n    decoded = dropoutType(dropoutRate)(codings)\n    decoded = Dense(block2.shape[1] * block2.shape[2] * block2.shape[3])(decoded)\n    decoded = tf.reshape(decoded, [-1, block2.shape[1], block2.shape[2], block2.shape[3]])\n    decoded = UpSampling2D(size= (1,8), interpolation= 'nearest', data_format= 'channels_last', name= 'AveragePooling2D_BE_Decoded')(decoded)\n    decoded = Activation('elu', name= 'ActivationB2_Decoded')(decoded)\n    decoded = BatchNormalization(name= 'BatchNormalizationB2_Decoded')(decoded)\n    decoded = Conv2DTranspose(F2, (1,1), use_bias = False, name = 'SSeparable_remplace_2_decoded')(decoded)\n    decoded = Conv2DTranspose(F1*D, (1,32), padding = 'same', name = 'Separable_remplace_1_decoded', use_bias = False)(decoded)\n\n    # DepthwiseDecoder\n    decoded = dropoutType(dropoutRate, name= 'DropoutTypeB1_Decoded')(decoded)\n    decoded = UpSampling2D(size= (1,4), interpolation= 'nearest', data_format= 'channels_last', name= 'AveragePooling2D_B1_Decoded')(decoded)\n    decoded = Activation('elu', name= 'ActivationB1_Decoded')(decoded)\n    decoded = BatchNormalization(name= 'BatchNormalizationB1.2_Decoded')(decoded)\n    decoded = Conv2DTranspose(F1*D , (Chans,1),\n                 name='Depth_wise_remplace_decoded',\n                 use_bias=False)(decoded)\n\n    # Conv2DDecoder\n    decoded = BatchNormalization(name= 'BatchNormalization1.1_Decoded')(decoded)\n    decoded = Conv2D(1 , (1,kernLength), padding='same',\n             name='Conv2D_1_Decoded',\n             use_bias=False)(decoded)\n\n    dense        = Dense(nb_classes, name = 'output',\n                         kernel_constraint = max_norm(norm_rate))(flatten_1)\n    softmax      = Activation('softmax', name = 'out_activation')(dense)\n\n    model = Model(inputs=input1, outputs= softmax)\n    \n    KL = -0.5 * tf.keras.backend.sum( 1 + codings_log_var - tf.keras.backend.exp(codings_log_var) - tf.keras.backend.square(codings_mean),axis=-1)\n    KL = 0.2*(tf.keras.backend.mean(KL)/codings_log_var.shape[-1])#Chans*Samples\n    model.add_loss(KL)#Chans*Samples)\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.684870Z","iopub.execute_input":"2024-10-07T19:37:48.685233Z","iopub.status.idle":"2024-10-07T19:37:48.712035Z","shell.execute_reply.started":"2024-10-07T19:37:48.685193Z","shell.execute_reply":"2024-10-07T19:37:48.711091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Load Dataset*","metadata":{}},{"cell_type":"markdown","source":"## *EEGNetVAE - Sbj: 43,14,6*","metadata":{}},{"cell_type":"code","source":"sbj=43\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:37:48.715214Z","iopub.execute_input":"2024-10-07T19:37:48.715530Z","iopub.status.idle":"2024-10-07T19:38:07.826715Z","shell.execute_reply.started":"2024-10-07T19:37:48.715498Z","shell.execute_reply":"2024-10-07T19:38:07.825757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Training*","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs43 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model43 = EEGNetVAE(nb_classes=2)\n        model43.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history43 = model43.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history43, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model43.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model43.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs43.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs43:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:38:07.828173Z","iopub.execute_input":"2024-10-07T19:38:07.828484Z","iopub.status.idle":"2024-10-07T19:46:05.755376Z","shell.execute_reply.started":"2024-10-07T19:38:07.828450Z","shell.execute_reply":"2024-10-07T19:46:05.754479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sbj=14\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:46:05.756466Z","iopub.execute_input":"2024-10-07T19:46:05.756753Z","iopub.status.idle":"2024-10-07T19:46:23.917427Z","shell.execute_reply.started":"2024-10-07T19:46:05.756715Z","shell.execute_reply":"2024-10-07T19:46:23.916409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs14 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model14 = EEGNetVAE(nb_classes=2)\n        model14.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history14 = model14.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history14, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model14.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model14.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs14.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs14:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:46:23.919143Z","iopub.execute_input":"2024-10-07T19:46:23.919535Z","iopub.status.idle":"2024-10-07T19:54:15.669623Z","shell.execute_reply.started":"2024-10-07T19:46:23.919491Z","shell.execute_reply":"2024-10-07T19:54:15.668621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sbj=6\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:54:15.671010Z","iopub.execute_input":"2024-10-07T19:54:15.671571Z","iopub.status.idle":"2024-10-07T19:54:33.710888Z","shell.execute_reply.started":"2024-10-07T19:54:15.671509Z","shell.execute_reply":"2024-10-07T19:54:33.709864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 5","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:54:33.712050Z","iopub.execute_input":"2024-10-07T19:54:33.712363Z","iopub.status.idle":"2024-10-07T19:54:33.716263Z","shell.execute_reply.started":"2024-10-07T19:54:33.712329Z","shell.execute_reply":"2024-10-07T19:54:33.715499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs6 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model6 = EEGNetVAE(nb_classes=2)\n        model6.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history6 = model6.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history6, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model6.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model6.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs6.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs6:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:54:33.717942Z","iopub.execute_input":"2024-10-07T19:54:33.718359Z","iopub.status.idle":"2024-10-07T20:02:49.518201Z","shell.execute_reply.started":"2024-10-07T19:54:33.718314Z","shell.execute_reply":"2024-10-07T20:02:49.517229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *EEGNet - sbj: 43,14,6*","metadata":{}},{"cell_type":"code","source":"sbj=43\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:27:50.471477Z","iopub.execute_input":"2024-10-07T20:27:50.472167Z","iopub.status.idle":"2024-10-07T20:28:08.059577Z","shell.execute_reply.started":"2024-10-07T20:27:50.472127Z","shell.execute_reply":"2024-10-07T20:28:08.058425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs43 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model43 = EEGNet(nb_classes=2)\n        model43.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history43 = model43.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history43, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model43.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model43.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs43.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs43:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:28:34.874152Z","iopub.execute_input":"2024-10-07T20:28:34.875299Z","iopub.status.idle":"2024-10-07T20:35:05.838346Z","shell.execute_reply.started":"2024-10-07T20:28:34.875256Z","shell.execute_reply":"2024-10-07T20:35:05.837498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sbj=14\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:39:24.997662Z","iopub.execute_input":"2024-10-07T20:39:24.998061Z","iopub.status.idle":"2024-10-07T20:39:41.739071Z","shell.execute_reply.started":"2024-10-07T20:39:24.998025Z","shell.execute_reply":"2024-10-07T20:39:41.738008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs14 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model14 = EEGNet(nb_classes=2)\n        model14.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history14 = model14.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history14, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model14.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model14.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs14.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs14:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:40:07.193250Z","iopub.execute_input":"2024-10-07T20:40:07.193910Z","iopub.status.idle":"2024-10-07T20:46:38.046867Z","shell.execute_reply.started":"2024-10-07T20:40:07.193871Z","shell.execute_reply":"2024-10-07T20:46:38.045897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sbj=6\n\nX , y = load_GIGA(sbj=sbj, **load_args)\n\nprint(\"===============================\")\nprint(X.shape)\nprint(y.shape)\nprint(\"===============================\")\nAcc_runs = []\n\nX_train_run_0 , y_train_run_0 = load_GIGA(sbj=sbj,run=0,**load_args)\nX_train_run_1 , y_train_run_1 = load_GIGA(sbj=sbj,run=1,**load_args)\nX_train_run_2 , y_train_run_2 = load_GIGA(sbj=sbj,run=2,**load_args)\nX_train_run_3 , y_train_run_3 = load_GIGA(sbj=sbj,run=3,**load_args)\nX_train_run_4 , y_train_run_4 = load_GIGA(sbj=sbj,run=4,**load_args)\n\nprint(\"===============================\")\nprint(\"Training X Run 0 data shape:\", X_train_run_0.shape)\nprint(y_train_run_0.shape)\nprint(\"Training X Run 1 data shape:\",X_train_run_1.shape)\nprint(y_train_run_1.shape)\nprint(\"Training X Run 2 data shape:\",X_train_run_2.shape)\nprint(y_train_run_2.shape)\nprint(\"Training X Run 3 data shape:\",X_train_run_3.shape)\nprint(y_train_run_3.shape)\nprint(\"Training X Run 4 data shape:\",X_train_run_4.shape)\nprint(y_train_run_4.shape)\nprint(\"===============================\")\n\nlist_train = [X_train_run_0,X_train_run_1,X_train_run_2,X_train_run_3,X_train_run_4]\n\nlist_y_train = [y_train_run_0,y_train_run_1,y_train_run_2,y_train_run_3,y_train_run_4]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:46:38.049025Z","iopub.execute_input":"2024-10-07T20:46:38.049494Z","iopub.status.idle":"2024-10-07T20:46:54.688139Z","shell.execute_reply.started":"2024-10-07T20:46:38.049448Z","shell.execute_reply":"2024-10-07T20:46:54.687152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Función para graficar loss y accuracy por cada fold\ndef plot_metrics(history, run_idx, fold_idx):\n    # Graficar Loss\n    plt.figure(figsize=(12, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Run {run_idx + 1} - Fold {fold_idx + 1} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.show()\n\n# Lista para almacenar las métricas de cada iteración\nAcc_runs6 = []\nn_folds = 5\n\n# Loop para la validación LORO, dejando un Run fuera para validación\nfor run_idx in range(len(list_train)):\n    print(f\"\\n=== Validación LORO con Run {run_idx + 1} excluido ===\")\n    \n    # Datos de test: el Run excluido\n    X_test = list_train[run_idx]\n    y_test = list_y_train[run_idx]\n    \n    # Datos de entrenamiento: todos los Runs excepto el excluido\n    X_train = np.concatenate([list_train[i] for i in range(len(list_train)) if i != run_idx], axis=0)\n    y_train = np.concatenate([list_y_train[i] for i in range(len(list_y_train)) if i != run_idx], axis=0)\n    \n    # Validación por Folds con shuffle\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold_idx + 1}/{n_folds} ---\")\n        \n        # Separar los datos en entrenamiento y validación para el Fold actual\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n        \n        # Crear el modelo (ajusta create_model según tu arquitectura)\n        model6 = EEGNet(nb_classes=2)\n        model6.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss={'out_activation': 'sparse_categorical_crossentropy'},\n                      loss_weights={'out_activation': 0.8}, metrics=['accuracy'])\n        \n        # Entrenar el modelo en los datos del fold actual\n        history6 = model6.fit(X_fold_train, y_fold_train,\n                            validation_data=(X_fold_val, y_fold_val),\n                            epochs=200,  # Ajusta el número de épocas según tus necesidades\n                            batch_size=16,\n                            verbose=0)\n        \n        # Graficar las métricas de loss y accuracy\n        plot_metrics(history6, run_idx, fold_idx)\n        \n        # Evaluar en el fold de validación\n        val_loss, val_acc = model6.evaluate(X_fold_val, y_fold_val)\n        print(f\"Fold {fold_idx + 1} - Val Loss: {val_loss}, Val Acc: {val_acc}\")\n        \n        # Evaluar en el Run de test (LORO)\n        test_loss, test_acc = model6.evaluate(X_test, y_test)\n        print(f\"Run {run_idx + 1} - Test Loss: {test_loss}, Test Acc: {test_acc}\")\n        \n        # Guardar resultados\n        Acc_runs6.append({\n            'run': run_idx + 1,\n            'fold': fold_idx + 1,\n            'val_loss': val_loss,\n            'val_acc': val_acc,\n            'test_loss': test_loss,\n            'test_acc': test_acc\n        })\n\n# Mostrar los resultados finales\nprint(\"\\n=== Resultados Finales ===\")\nfor result in Acc_runs6:\n    print(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:46:54.689780Z","iopub.execute_input":"2024-10-07T20:46:54.690344Z","iopub.status.idle":"2024-10-07T20:54:10.911279Z","shell.execute_reply.started":"2024-10-07T20:46:54.690296Z","shell.execute_reply":"2024-10-07T20:54:10.910352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n# ----------------------------------------------------------------------\ndef agco(method_1, method_2, ticks, labels, sort='method_2', reference_c='C1', gain_c='C0', loss_c='C3', barwidth=6, ylabel='Accuracy [%]', xlabel='Subjects', gain_labels=['gain', 'loss'], fig=None, ax=None, size=(15, 5), dpi=90, **kwargs):\n    \"\"\"\"\"\"\n    if 'lose_c' in kwargs:\n        loss_c = kwargs['lose_c']\n\n    if fig is None:\n        plt.figure(figsize=size, dpi=dpi)\n\n    if ax is None:\n        ax = plt.subplot(111)\n\n\n    if sort is None:\n        index = np.arange(len(method_2))\n    elif sort == 'method_1':\n        index = np.argsort(method_1)[::-1]\n    elif sort == 'method_1r':\n        index = np.argsort(method_1)\n    elif sort == 'method_2':\n        index = np.argsort(method_2)[::-1]\n    elif sort == 'method_2r':\n        index = np.argsort(method_2)\n    else:\n        index = sort\n\n    colors = np.array(method_2[index]\n                      - method_1[index] < 0, dtype=np.object_)\n\n    index = range(len(method_1))\n    \n    if sort=='method_2':\n        p1, = plt.plot(method_2[index], color=reference_c, linestyle='--',)\n        p2, = plt.plot(method_1[index], color=gain_c,\n                       linestyle='--', alpha=0.3)\n        colors[colors == 0] = loss_c\n        colors[colors == 1] = gain_c\n    else:\n        p1, = plt.plot(method_1[index], color=reference_c, linestyle='--',)\n        p2, = plt.plot(method_2[index], color=gain_c,\n                       linestyle='--', alpha=0.3)\n        colors[colors == 0] = gain_c\n        colors[colors == 1] = loss_c\n\n    plots = [p1, p2]\n\n    if np.array([colors == gain_c]).any():\n        p3 = plt.vlines(np.array(sorted(index))[colors == gain_c], method_1[index][colors == gain_c],\n                        method_2[index][colors == gain_c], color=colors[colors == gain_c], linewidth=barwidth)\n        labels.append(gain_labels[0])\n        plots.append(p3)\n\n    if np.array([colors == loss_c]).any():\n        p4 = plt.vlines(np.array(sorted(index))[colors == loss_c], method_1[index][colors == loss_c],\n                        method_2[index][colors == loss_c], color=colors[colors == loss_c], linewidth=barwidth)\n        labels.append(gain_labels[1])\n        plots.append(p4)\n\n    plt.ylabel(ylabel, fontsize=15)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.xticks(range(len(ticks)), ticks[index], rotation=90)\n\n    try:\n        ax.spines.right.set_visible(False)\n        ax.spines.top.set_visible(False)\n    except:\n        pass\n\n    try:\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n    except:\n        pass\n\n    l1 = plt.legend(plots, labels, loc='upper center',\n                    ncol=2, bbox_to_anchor=(0.5, 1), fontsize=12)\n    plt.gca().add_artist(l1)\n\n    return plt.gcf()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:22:44.901199Z","iopub.execute_input":"2024-10-07T22:22:44.901563Z","iopub.status.idle":"2024-10-07T22:22:44.923063Z","shell.execute_reply.started":"2024-10-07T22:22:44.901527Z","shell.execute_reply":"2024-10-07T22:22:44.922201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nticks_T2 = np.array(['R1F1', 'R1F2', 'R1F3', 'R1F4', 'R1F5', 'R2F1', 'R2F2', 'R2F3', 'R2F4', 'R2F5', 'R3F1', 'R3F2', 'R3F3', 'R3F4', 'R3F5', 'R4F1', 'R4F2', 'R4F3', 'R4F4', 'R4F5', 'R5F1' ,'R5F2' ,'R5F3', 'R5F4', 'R5F5'])\n\nmethod_1T2 = np.array([\n    0.574999988079071,  # Run 1 Fold 1\n    0.5,                # Run 1 Fold 2\n    0.824999988079071,  # Run 1 Fold 3\n    0.5,                # Run 1 Fold 4\n    0.675000011920929,  # Run 1 Fold 5\n    0.925000011920929,  # Run 2 Fold 1\n    0.8999999761581421, # Run 2 Fold 2\n    0.75,               # Run 2 Fold 3\n    0.9750000238418579, # Run 2 Fold 4\n    1.0,                # Run 2 Fold 5\n    1.0,                # Run 3 Fold 1\n    0.8500000238418579, # Run 3 Fold 2\n    1.0,                # Run 3 Fold 3\n    0.8500000238418579, # Run 3 Fold 4\n    0.925000011920929,  # Run 3 Fold 5\n    0.9750000238418579, # Run 4 Fold 1\n    1.0,                # Run 4 Fold 2\n    0.9750000238418579, # Run 4 Fold 3\n    1.0,                # Run 4 Fold 4\n    0.949999988079071,  # Run 4 Fold 5\n    0.9750000238418579, # Run 5 Fold 1\n    0.875,              # Run 5 Fold 2\n    0.949999988079071,  # Run 5 Fold 3\n    0.824999988079071,  # Run 5 Fold 4\n    1.0                 # Run 5 Fold 5\n])*100\n\nmethod_2T2 = np.array([\n    0.6499999761581421,  # run 1, fold 1\n    0.675000011920929,   # run 1, fold 2\n    0.7250000238418579,  # run 1, fold 3\n    0.6499999761581421,  # run 1, fold 4\n    0.574999988079071,   # run 1, fold 5\n    0.9750000238418579,  # run 2, fold 1\n    0.9750000238418579,  # run 2, fold 2\n    0.9750000238418579,  # run 2, fold 3\n    0.8500000238418579,  # run 2, fold 4\n    0.8500000238418579,  # run 2, fold 5\n    1.0,                 # run 3, fold 1\n    1.0,                 # run 3, fold 2\n    1.0,                 # run 3, fold 3\n    0.9750000238418579,  # run 3, fold 4\n    1.0,                 # run 3, fold 5\n    0.949999988079071,   # run 4, fold 1\n    0.9750000238418579,  # run 4, fold 2\n    1.0,                 # run 4, fold 3\n    0.949999988079071,   # run 4, fold 4\n    1.0,                 # run 4, fold 5\n    0.949999988079071,   # run 5, fold 1\n    0.9750000238418579,  # run 5, fold 2\n    0.925000011920929,   # run 5, fold 3\n    0.8999999761581421,  # run 5, fold 4\n    0.8500000238418579   # run 5, fold 5\n])*100\n\n\nticks_T3 = np.array(['R1F1' 'R1F2' 'R1F3' 'R1F4' 'R1F5' 'R2F1' 'R2F2' 'R2F3' 'R2F4' 'R2F5' 'R3F1' 'R3F2' 'R3F3' 'R3F4' 'R3F5' 'R4F1' 'R4F2' 'R4F3' 'R4F4' 'R4F5' 'R5F1' 'R5F2' 'R5F3' 'R5F4' 'R5F5'])\n\nmethod_1T3 = np.array([\n    0.9750000238418579,  # Run 1 Fold 1\n    0.9750000238418579,  # Run 1 Fold 2\n    0.9750000238418579,  # Run 1 Fold 3\n    0.9750000238418579,  # Run 1 Fold 4\n    0.9750000238418579,  # Run 1 Fold 5\n    0.8750000000000000,  # Run 2 Fold 1\n    0.8500000238418579,  # Run 2 Fold 2\n    0.8750000000000000,  # Run 2 Fold 3\n    0.8750000000000000,  # Run 2 Fold 4\n    0.949999988079071,   # Run 2 Fold 5\n    0.9750000238418579,  # Run 3 Fold 1\n    0.9750000238418579,  # Run 3 Fold 2\n    0.9750000238418579,  # Run 3 Fold 3\n    0.925000011920929,   # Run 3 Fold 4\n    0.949999988079071,   # Run 3 Fold 5\n    0.925000011920929,   # Run 4 Fold 1\n    0.8750000000000000,  # Run 4 Fold 2\n    0.8999999761581421,  # Run 4 Fold 3\n    0.8999999761581421,  # Run 4 Fold 4\n    0.8750000000000000,  # Run 4 Fold 5\n    0.925000011920929,   # Run 5 Fold 1\n    0.949999988079071,   # Run 5 Fold 2\n    0.925000011920929,   # Run 5 Fold 3\n    0.8999999761581421,  # Run 5 Fold 4\n    0.949999988079071    # Run 5 Fold 5\n])*100\nmethod_2T3 = np.array([\n    1.0,                # run 1, fold 1\n    1.0,                # run 1, fold 2\n    1.0,                # run 1, fold 3\n    1.0,                # run 1, fold 4\n    1.0,                # run 1, fold 5\n    0.949999988079071, # run 2, fold 1\n    0.949999988079071, # run 2, fold 2\n    0.8999999761581421, # run 2, fold 3\n    1.0,                # run 2, fold 4\n    0.9750000238418579, # run 2, fold 5\n    0.9750000238418579, # run 3, fold 1\n    0.9750000238418579, # run 3, fold 2\n    0.9750000238418579, # run 3, fold 3\n    1.0,                # run 3, fold 4\n    1.0,                # run 3, fold 5\n    0.925000011920929, # run 4, fold 1\n    0.925000011920929, # run 4, fold 2\n    0.925000011920929, # run 4, fold 3\n    0.925000011920929, # run 4, fold 4\n    0.925000011920929, # run 4, fold 5\n    0.8500000238418579, # run 5, fold 1\n    0.8999999761581421, # run 5, fold 2\n    0.875,             # run 5, fold 3\n    0.8999999761581421, # run 5, fold 4\n    0.925000011920929  # run 5, fold 5\n])*100\n\nticks_T4 = np.array(['R1F1' 'R1F2' 'R1F3' 'R1F4' 'R1F5' 'R2F1' 'R2F2' 'R2F3' 'R2F4' 'R2F5' 'R3F1' 'R3F2' 'R3F3' 'R3F4' 'R3F5' 'R4F1' 'R4F2' 'R4F3' 'R4F4' 'R4F5' 'R5F1' 'R5F2' 'R5F3' 'R5F4' 'R5F5'])\n\n\nmethod_1T4 = np.array([\n    0.6111111044883728,  # Run 1 Fold 1\n    0.4444444477558136,  # Run 1 Fold 2\n    0.5555555820465088,  # Run 1 Fold 3\n    0.4166666567325592,  # Run 1 Fold 4\n    0.4722222089767456,  # Run 1 Fold 5\n    0.8055555820465088,  # Run 2 Fold 1\n    0.8888888955116272,  # Run 2 Fold 2\n    0.9444444179534912,  # Run 2 Fold 3\n    0.5833333134651184,  # Run 2 Fold 4\n    0.8611111044883728,  # Run 2 Fold 5\n    0.9411764740943909,  # Run 3 Fold 1\n    0.970588207244873,   # Run 3 Fold 2\n    0.970588207244873,   # Run 3 Fold 3\n    0.9411764740943909,  # Run 3 Fold 4\n    0.970588207244873,   # Run 3 Fold 5\n    0.7222222089767456,  # Run 4 Fold 1\n    1.0,                 # Run 4 Fold 2\n    1.0,                 # Run 4 Fold 3\n    0.9722222089767456,  # Run 4 Fold 4\n    0.8333333134651184,  # Run 4 Fold 5\n    0.5277777910232544,  # Run 5 Fold 1\n    0.5,                 # Run 5 Fold 2\n    0.6111111044883728,  # Run 5 Fold 3\n    0.4166666567325592,  # Run 5 Fold 4\n    0.5833333134651184   # Run 5 Fold 5\n])*100\nmethod_2T4 = np.array([\n    0.4444444477558136,  # Run 1 Fold 1\n    0.4444444477558136,  # Run 1 Fold 2\n    0.5277777910232544,  # Run 1 Fold 3\n    0.3055555522441864,  # Run 1 Fold 4\n    0.5,                 # Run 1 Fold 5\n    0.9444444179534912,  # Run 2 Fold 1\n    0.9444444179534912,  # Run 2 Fold 2\n    0.9166666865348816,  # Run 2 Fold 3\n    0.9444444179534912,  # Run 2 Fold 4\n    0.8888888955116272,  # Run 2 Fold 5\n    0.970588207244873,    # Run 3 Fold 1\n    0.970588207244873,    # Run 3 Fold 2\n    0.7647058963775635,  # Run 3 Fold 3\n    0.970588207244873,    # Run 3 Fold 4\n    0.970588207244873,    # Run 3 Fold 5\n    0.9722222089767456,  # Run 4 Fold 1\n    0.9444444179534912,  # Run 4 Fold 2\n    0.9444444179534912,  # Run 4 Fold 3\n    0.9722222089767456,  # Run 4 Fold 4\n    0.9444444179534912,  # Run 4 Fold 5\n    0.5,                 # Run 5 Fold 1\n    0.6666666865348816,  # Run 5 Fold 2\n    0.5833333134651184,  # Run 5 Fold 3\n    0.6388888955116272,  # Run 5 Fold 4\n    0.6388888955116272   # Run 5 Fold 5\n])*100","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:25:14.204940Z","iopub.execute_input":"2024-10-07T22:25:14.205282Z","iopub.status.idle":"2024-10-07T22:25:14.228260Z","shell.execute_reply.started":"2024-10-07T22:25:14.205250Z","shell.execute_reply":"2024-10-07T22:25:14.227322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ticks_T2)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:25:14.698761Z","iopub.execute_input":"2024-10-07T22:25:14.699574Z","iopub.status.idle":"2024-10-07T22:25:14.705381Z","shell.execute_reply.started":"2024-10-07T22:25:14.699527Z","shell.execute_reply":"2024-10-07T22:25:14.704489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = agco(method_1T2, method_2T2, ticks_T2, labels= ['EEGNet', 'VAEEEGNet'], size=(25, 10), gain_labels=['loss', 'gain'])\n\npastel2 = plt.get_cmap('Pastel2')\n#f3 = plt.fill_betweenx([35, 100], 39.5, 50, color=pastel2(1), alpha=0.8, label='hola')\n#f2 = plt.fill_betweenx([35, 100], 16.5, 39.5, color=pastel2(5), alpha=0.8)\n#f1 = plt.fill_betweenx([35, 100], -1, 16.5, color=pastel2(0), alpha=0.8)\n#l2 = plt.legend([f1, f2, f3], [\"Group 1\", 'Group 2', 'Group 3'], loc=4, ncol=1, bbox_to_anchor =(1.07, 0.5), fontsize=12)\n\nplt.savefig('accuracies.pdf', bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:25:14.997976Z","iopub.execute_input":"2024-10-07T22:25:14.998275Z","iopub.status.idle":"2024-10-07T22:25:16.259680Z","shell.execute_reply.started":"2024-10-07T22:25:14.998244Z","shell.execute_reply":"2024-10-07T22:25:16.258832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = agco(method_1T3, method_2T3, ticks_T2, labels= ['EEGNet', 'VAEEEGNet'], size=(25, 10), gain_labels=['loss', 'gain'])\n\npastel2 = plt.get_cmap('Pastel2')\n#f3 = plt.fill_betweenx([35, 100], 39.5, 50, color=pastel2(1), alpha=0.8, label='hola')\n#f2 = plt.fill_betweenx([35, 100], 16.5, 39.5, color=pastel2(5), alpha=0.8)\n#f1 = plt.fill_betweenx([35, 100], -1, 16.5, color=pastel2(0), alpha=0.8)\n#l2 = plt.legend([f1, f2, f3], [\"Group 1\", 'Group 2', 'Group 3'], loc=4, ncol=1, bbox_to_anchor =(1.07, 0.5), fontsize=12)\n\nplt.savefig('accuracies.pdf', bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:26:11.779314Z","iopub.execute_input":"2024-10-07T22:26:11.779707Z","iopub.status.idle":"2024-10-07T22:26:12.875944Z","shell.execute_reply.started":"2024-10-07T22:26:11.779664Z","shell.execute_reply":"2024-10-07T22:26:12.874983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = agco(method_1T4, method_2T4, ticks_T2, labels= ['EEGNet', 'VAEEEGNet'], size=(25, 10), gain_labels=['loss', 'gain'])\n\npastel2 = plt.get_cmap('Pastel2')\n#f3 = plt.fill_betweenx([35, 100], 39.5, 50, color=pastel2(1), alpha=0.8, label='hola')\n#f2 = plt.fill_betweenx([35, 100], 16.5, 39.5, color=pastel2(5), alpha=0.8)\n#f1 = plt.fill_betweenx([35, 100], -1, 16.5, color=pastel2(0), alpha=0.8)\n#l2 = plt.legend([f1, f2, f3], [\"Group 1\", 'Group 2', 'Group 3'], loc=4, ncol=1, bbox_to_anchor =(1.07, 0.5), fontsize=12)\n\nplt.savefig('accuracies.pdf', bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T22:26:25.369717Z","iopub.execute_input":"2024-10-07T22:26:25.370625Z","iopub.status.idle":"2024-10-07T22:26:26.647680Z","shell.execute_reply.started":"2024-10-07T22:26:25.370582Z","shell.execute_reply":"2024-10-07T22:26:26.646724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}